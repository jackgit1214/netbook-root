package com.crawler4j;

import java.util.List;

import org.junit.Test;

import edu.uci.ics.crawler4j.crawler.CrawlConfig;
import edu.uci.ics.crawler4j.crawler.CrawlController;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;

public class LocalDataCollectorCrawlerTest {

	@Test
	public void test() throws Exception {

      String rootFolder = "/data";
      int numberOfCrawlers = 4;

      CrawlConfig config = new CrawlConfig();
      config.setCrawlStorageFolder(rootFolder);
      config.setMaxPagesToFetch(10);
      config.setPolitenessDelay(1000);

      PageFetcher pageFetcher = new PageFetcher(config);
      RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
      RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
      CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);

      controller.addSeed("http://www.ics.uci.edu/");
      controller.start(LocalDataCollectorCrawler.class, numberOfCrawlers);

      List<Object> crawlersLocalData = controller.getCrawlersLocalData();
      long totalLinks = 0;
      long totalTextSize = 0;
      int totalProcessedPages = 0;
      for (Object localData : crawlersLocalData) {
              CrawlStat stat = (CrawlStat) localData;
              totalLinks += stat.getTotalLinks();
              totalTextSize += stat.getTotalTextSize();
              totalProcessedPages += stat.getTotalProcessedPages();
      }
      System.out.println("Aggregated Statistics:");
      System.out.println("   Processed Pages: " + totalProcessedPages);
      System.out.println("   Total Links found: " + totalLinks);
      System.out.println("   Total Text Size: " + totalTextSize);
	}

}
